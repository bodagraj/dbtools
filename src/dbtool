#!/usr/bin/python3
#import multiprocessing as mp
import logging
import threading
import time
from logging.handlers import RotatingFileHandler
import os
import subprocess
import shlex
import tempfile
import pprint
import json
import sys
import argparse
import teradatasql
import pandas as pd
import sqlalchemy
import warnings
import re



#print("Number of processors: ", mp.cpu_count())


def setup_logging(arg):
	
	"""
	Setup a default log file to /tmp/filename.log
	logfile will be rotated if it crosses 500MB upto maximum of 4 recent logs
	Format prints as follows
	2022-01-27 02:16:09,713 - [DEBUG] - [Thread-1] - root - (snowtool.py).monitor_worker(12) - Hi from myfunc
	2022-01-27 02:16:09,714 - [DEBUG] - [MainThread] - root - (snowtool.py).main(38) - Hello from main
	"""
	
	prog_name = os.path.basename(os.path.realpath(__file__))
	fname = tempfile.gettempdir() +'/'+ prog_name + '.log'
	logging.basicConfig(
		level=logging.DEBUG,
		format="%(asctime)s - [%(levelname)s] - [%(threadName)s] - %(name)s - (%(filename)s).%(funcName)s(%(lineno)d) - %(message)s",
		handlers=[RotatingFileHandler(filename=fname,mode='w', maxBytes=512000, backupCount=4)]
	)
	print(" * check detailed progress in [",fname,"] hit cntrl+c to terminate the program")

		
def main(arg):

	"""
	Multi threaded program it uses multithreading rater than multiprocessing, as the logic of this progra is not compute intesnive.
	"""
	my_arg = pprint.pformat(arg)
	logging.info("input arguments are : \n"+my_arg)

	try:
		logging.debug('Starting main program...')
		
		if 'copy_data' in arg['action'].keys():
			copy_data_args = arg['action']['copy_data']
			handle_copy_data(copy_data_args)
		else:
			msg = "input configuration doesn't defined any action, exiting!"
			print(msg)
			logging.critical(msg)	
			sys.exit(-1)
	except KeyboardInterrupt:
		info['stop'] = True
		logging.critical("keyboard inturrupt recieved terminating the program...",info)
		sys.exit(-1)
	except Exception as e:
		logging.critical(e)
		print("Oops!,", e.__class__, "occurred.")
		sys.exit(-1)
	
	## check final status
	msg = " * Program completed, check logs for more details"
	logging.info(msg)
	print(msg)
	sys.exit(0)

def handle_copy_data(arg):
	logging.info('Processing action=copy_data')
	threads = []
	### monitor process
	info = {'stop': False}
	thread = threading.Thread(target=monitor_worker, args=(arg,))
	thread.start()
	threads.append(thread)
	
	### process source
	thread_source = threading.Thread(target=teradata_export, args=(arg,))
	thread_source.start()
	threads.append(thread_source)
	
	### copy to s3 staging area
	thread_s3_copy = threading.Thread(target=copy_to_s3, args=(arg,))
	thread_s3_copy.start()
	threads.append(thread_s3_copy)
	
	
	# join all threads
	for t in threads:
		t.join()

def copy_to_s3(arg):
	my_arg = pprint.pformat(arg)
	logging.info("input arguments are : \n"+my_arg)
"""	
	for source_file in source_files:
		
	while True:
		
		return_code = is_file_ready()
		
		if return_code is not None:
			logging.info('RETURN CODE:'+ str(return_code))
			# Process has finished, read rest of the output
			out = "".join(process.stdout.readlines())
			err = "".join(process.stderr.readlines())
			logging.info("Command returned\n"+"\nout ==> "+out)
			if err != '':
				logging.info("Command returned\n"+"\nerr ==> "+err)
				print("ERROR ==> "+err)
			
			if return_code != 0:
				logging.critical('Command failed, please check messages above')
			
			break
"""

def teradata_export(arg):
	my_arg = pprint.pformat(arg)
	logging.info("input arguments are : \n"+my_arg)
	logging.debug('connecting to source:'+arg['source']['dbhost'])
	connect = teradata_connect(arg['source'])
	arg['source']['connect'] = connect

	
	table_dic = teradata_get_table_details(arg['source'])
	table_details = pprint.pformat(table_dic)
	logging.info("Table Details are : \n"+table_details)
	#print(table_details)
	
	fexp_cmd = teradata_get_fexp_command({'table_dic' : table_dic,'inputs' : arg})
	my_pretty = pprint.pformat(fexp_cmd)
	logging.debug(" * Teradata Fast Export commands: "+my_pretty)
	
	### sequential for now
	export_status = teradata_execute_export(fexp_cmd)
	my_arg = pprint.pformat(export_status)
	logging.info("Export Status : \n"+my_arg)
	return export_status

def teradata_execute_export(arg):
	
	for table in arg:
		cmd_args = { 'cmd' : 'tbuild'+' -f '+arg[table] + ' -C ' }
		
		print(f""" * Exporting table {table}...""")
		stats = execute_command(cmd_args)
		status = {}
		return_code = stats['return_code']
		output = stats['output']
		
		### export status
		export_status = 'unknown'
		exported_rows = None
		discarded_rows = None
		for m in re.finditer(r"\bJob\sstep\sMAIN_STEP\scompleted\ssuccessfully\b", output):
			if m.group(0):
				export_status = m.group(0)
				logging.debug('Export status for table '+table+':'+export_status)
				export_status = 'ok'
			else:
				export_status = 'not_ok'
				msg = " * Export Failed for table : "+table+', Please check logs for more details'
				print(msg)
				logging.critical(msg)
				sys.exit(-1)
		### export rows
		for m in re.finditer(r"\s+Total\s+Rows\s+Exported\:\s+(\d+)", output):
			if m.group(0):
				exported_rows = m.group(1)
				logging.debug('Exported rows for table '+table+':'+exported_rows)
				
			else:
				export_status = 'not_ok'
				msg = " * Export Failed for table : "+table+', Please check logs for more details'
				print(msg)
				logging.critical(msg)
				sys.exit(-1)
		### discarded_rows
		for m in re.finditer(r"\s+Total\s+Rows\s+Discarded\:\s(\d+)", output):
			if m.group(0):
				discarded_rows = m.group(1)
				logging.debug('Discarded rows for table '+table+':'+discarded_rows)
			else:
				msg = " * Export Failed for table : "+table+', Please check logs for more details'
				print(msg)
				logging.critical(msg)
				sys.exit(-1)
		
		print(f""" * Export status [{export_status}], Exported rows for table {table} : [{exported_rows}]""")
		status[table] = {
			'return_code' : return_code,
			'export_status' : export_status,
			'exported_rows' : exported_rows,
			'discarded_rows' : discarded_rows
		}
		
	return status

def teradata_get_fexp_command(arg):
	table_dic = arg['table_dic']
	args = arg['inputs'] 
	
	dbuser = args['source']['user']
	dbpass = args['source']['password']
	dbhost = args['source']['dbhost']
	export_dir = args['export_dir']
	delimiter = args['delimiter']
	parallel_writers = args['parallel_writers']
	parallel_readers = args['parallel_readers']
	
	fexp_command = None
	fexp_commands = {}
	
	for table in table_dic:
		
		col_list = []
		select_col_list = []
		for colid in sorted(table_dic[table].keys()):
			name = table_dic[table][colid]['ColumnName']
			type = table_dic[table][colid]['ColumnType']
			size = table_dic[table][colid]['ColumnLength']
			tb = table_dic[table][colid]['TableName']
			external_datatype_with_size = table_dic[table][colid]['ExternalDatatypeSize']
		
			select_col = f""" "{name}" {external_datatype_with_size}"""
			exp_select_col = f"""cast("{name}" as {external_datatype_with_size})"""
			
			select_col_list.append(exp_select_col)
			col_list.append(select_col)
		
		name_type_size_all = "\n\t,".join(col_list)
		name_type_size_all = "\n\t"+name_type_size_all
		exp_select_col_list = ",".join(select_col_list)
		sql = f"""select {exp_select_col_list} from {table}"""
		
		#print(name_type_size_all)
		fexp_command = f"""
DEFINE JOB EXPORT_{tb}_DATA
DESCRIPTION 'Exports {tb} data to a formatted file using EXPORT'
(
    DEFINE OPERATOR Consumer_File_Detail
    DESCRIPTION 'Defining a consumer operator for storing retrieved data to a file'
    TYPE DATACONNECTOR CONSUMER
    SCHEMA *
    ATTRIBUTES
    (
        VARCHAR DirectoryPath='{export_dir}',
        VARCHAR FileName='{tb}.out.gz',
        VARCHAR FORMAT= 'DELIMITED',
        VARCHAR OpenMode='Write',
        VARCHAR TextDelimiter='{delimiter}'
    );
    
    DEFINE SCHEMA Define_{tb}_Schema
    DESCRIPTION 'Defining a Schema to describe the structure of the output file'
    (
		{name_type_size_all}
    );
    
    DEFINE OPERATOR Producer_Query_Detail
    TYPE EXPORT
    SCHEMA Define_{tb}_Schema
    ATTRIBUTES
    (
       VARCHAR UserName='{dbuser}',
       VARCHAR UserPAssword='{dbpass}',
       VARCHAR SelectStmt = '{sql};',
       VARCHAR Tdpid='{dbhost}',
       INTEGER MaxSessions=6,
       INTEGER minsessions=2,
       INTEGER TenacityHours=2,
       INTEGER TenacitySleep=10,
       VARCHAR QueryBandSessInfo='Action=TPT_EXPORT;Format=TEXT;Application=dbtool;'

    );
    
    APPLY TO OPERATOR( Consumer_File_Detail[{parallel_writers}] )
    SELECT * FROM OPERATOR( Producer_Query_Detail[{parallel_readers}] );
);
"""
		fexp_file = f"""{export_dir}/{table}.fexp"""
		logging.debug("Fast export file : "+fexp_file)
		logging.debug("Fast Export Command : "+fexp_command)
		try:
			with open(fexp_file, 'w') as f:
				f.write(fexp_command)
				logging.debug(fexp_command)
		except Exception as e:
			logging.critical('Failed to create fast export file'+format(e))
		#print(" * Fast Export File : ",fexp_file)
		fexp_commands[table] = fexp_file
	
	return fexp_commands

def teradata_get_table_details(arg):

	db = arg['dbname']
	connect = arg['connect']
	
	sql = f"""
		select
			trim(t.DatabaseName) as DatabaseName,
			trim(t.TableName) as TableName,
			trim(t.TableKind) as TableKind,
			trim(c.ColumnName) as ColumnName,
			case 
				when trim(c.ColumnType) = 'AT' then 'varchar(15)'
				when trim(c.ColumnType) = 'BF' then 'varchar'||trim(c.ColumnLength)||')'
				when trim(c.ColumnType) = 'BO' then 'varchar('||trim(c.ColumnLength)||')'
				when trim(c.ColumnType) = 'BV' then 'varchar'||trim(c.ColumnLength)||')'
				when trim(c.ColumnType) = 'D' then 'varchar('||trim(DecimalTotalDigits+DecimalFractionalDigits)||')'
				when trim(c.ColumnType) = 'CF' then 'varchar('||trim(c.ColumnLength)||')'
				when trim(c.ColumnType) = 'CV' then 'varchar('||trim(c.ColumnLength)||')'
				when trim(c.ColumnType) = 'DA' then 'varchar(10)'
				when trim(c.ColumnType) = 'DH' then 'varchar(8)'
				when trim(c.ColumnType) = 'DM' then 'varchar(11)'
				when trim(c.ColumnType) = 'DS' then 'varchar(21)'
				when trim(c.ColumnType) = 'DY' then 'varchar(10)'
				when trim(c.ColumnType) = 'F' then 'varchar(20)'
				when trim(c.ColumnType) = 'HM' then 'varchar(8)'
				when trim(c.ColumnType) = 'HR' then 'varchar(8)'
				when trim(c.ColumnType) = 'HS' then 'varchar(18)'
				when trim(c.ColumnType) = 'I' then 'varchar(10)'
				when trim(c.ColumnType) = 'I1' then 'varchar(3)'
				when trim(c.ColumnType) = 'I2' then 'varchar(6)'
				when trim(c.ColumnType) = 'I8' then 'varchar(20)'
				when trim(c.ColumnType) = 'MI' then 'varchar(5)'
				when trim(c.ColumnType) = 'MO' then 'varchar(5)'
				when trim(c.ColumnType) = 'MS' then 'varchar(15)'
				when trim(c.ColumnType) = 'TS' then 'varchar(26)'
				when trim(c.ColumnType) = 'SZ' then 'varchar(32)'
				when trim(c.ColumnType) = 'TZ' then 'varchar(32)'
				when trim(c.ColumnType) = 'YM' then 'varchar(8)'
				when trim(c.ColumnType) = 'YR' then 'varchar(5)'
				when trim(c.ColumnType) = 'PM' then 'varchar(32)'
			end as ExternalDatatypeSize,
			case 
				when trim(c.ColumnType) = 'AT' then 'time'
				when trim(c.ColumnType) = 'BF' then 'byte'
				when trim(c.ColumnType) = 'BO' then 'blob'
				when trim(c.ColumnType) = 'BV' then 'varbyte'
				when trim(c.ColumnType) = 'CF' then 'char'
				when trim(c.ColumnType) = 'CV' then 'varchar'
				when trim(c.ColumnType) = 'D' then 'decimal'
				when trim(c.ColumnType) = 'DA' then 'date'
				when trim(c.ColumnType) = 'DH' then 'interval day to hour'
				when trim(c.ColumnType) = 'DM' then 'interval day to minute'
				when trim(c.ColumnType) = 'DS' then 'interval day to second'
				when trim(c.ColumnType) = 'DY' then 'interval day'
				when trim(c.ColumnType) = 'F' then 'float'
				when trim(c.ColumnType) = 'HM' then 'interval hour to minute'
				when trim(c.ColumnType) = 'HR' then 'interval hour'
				when trim(c.ColumnType) = 'HS' then 'interval hour to second'
				when trim(c.ColumnType) = 'I' then 'integer'
				when trim(c.ColumnType) = 'I1' then 'byteint'
				when trim(c.ColumnType) = 'I2' then 'smallint'
				when trim(c.ColumnType) = 'I8' then 'bigint'
				when trim(c.ColumnType) = 'MI' then 'interval minute'
				when trim(c.ColumnType) = 'MO' then 'interval month'
				when trim(c.ColumnType) = 'MS' then 'interval minute to second'
				when trim(c.ColumnType) = 'TS' then 'timestamp'
				when trim(c.ColumnType) = 'SZ' then 'timestamp with time zone'
				when trim(c.ColumnType) = 'TZ' then 'time with time zone'
				when trim(c.ColumnType) = 'YM' then 'interval year to month'
				when trim(c.ColumnType) = 'YR' then 'interval year'
				when trim(c.ColumnType) = 'PM' then 'temporal'
			end as ColumnType,
			trim(c.ColumnLength) as ColumnLength,
			trim(c.Nullable) as Nullable,
			trim(c.DecimalTotalDigits) DecimalTotalDigits,
			trim(c.DecimalFractionalDigits) DecimalFractionalDigits,
			cast(trim(c.ColumnId) as integer) ColumnId
		from
			dbc.tables t inner join dbc.columns c on t.TableName=c.TableName and t.DatabaseName=c.DatabaseName
		where t.DatabaseName='{db}'
		order by t.DatabaseName,t.TableName,c.ColumnId
		"""
	
	table_dic = {}
	df = None
	logging.debug('executing query:'+sql)
	try:
		warnings.filterwarnings("ignore")
		df = pd.read_sql(sql,connect)
	except Exception as e:
		logging.critical(e)
		sys.exit(-1)
	
	df = df.reset_index()  # make sure indexes pair with number of rows
	
	for index, row in df.iterrows():
		table_name = row['DatabaseName']+'.'+row['TableName']
		col_id = row['ColumnId']
		col_details = {
				'DatabaseName' : row['DatabaseName'],
				'TableName' : row['TableName'],
				'TableKind' : row['TableKind'],
				'ColumnName' : row['ColumnName'],
				'ColumnType' : row['ColumnType'],
				'ColumnLength' : row['ColumnLength'],
				'Nullable' : row['Nullable'],
				'DecimalTotalDigits' : row['DecimalTotalDigits'],
				'DecimalFractionalDigits' : row['DecimalFractionalDigits'],
				'ColumnId' : row['ColumnId'],
				'ExternalDatatypeSize' : row['ExternalDatatypeSize']
		}
		cold = {col_id : col_details}
		
		try:
			#table_dic[table_name][col_id].append(col_details)
			table_dic[table_name][col_id] = col_details
		except Exception as e:
			### if its new column then initialise new column only
			if table_name in table_dic.keys() and col_id not in table_dic[table_name].keys():
				table_dic[table_name][col_id] = {}
				table_dic[table_name][col_id] = col_details
			else:
				### if its new then initialise and add
				table_dic[table_name] = {}
				table_dic[table_name][col_id] = {}
				table_dic[table_name][col_id] = col_details
	
	return table_dic
	
	
def teradata_connect(arg):
	
	dbhost = arg['dbhost']
	dbuser = arg['user']
	dbpass = arg['password']
	
	conn = None
	
	try:
		conn = teradatasql.connect(host=dbhost, user=dbuser, password=dbpass)
		#data = pd.read_sql('select top 5 * from dbc.tables;', conn)
	except Exception as e:
		logging.critical(e)
		sys.exit(-1)
	
	msg = "Successfully made connection to "+dbhost
	logging.info(msg)
	return conn

def monitor_worker(arg):
	
	'''
	Monitor all threads/processors
	'''
	my_arg = pprint.pformat(arg)
	logging.info("input arguments are : \n"+my_arg)
	
	cmd_args = {
				'cmd' : 'ping -c 4 python.org'
				}
	
	status = None
	try:
		status = execute_command(cmd_args)
	except Exception as ex:
		json_string = json.dumps(status)
		logging.critical(ex)
		logging.critical(json_string+"\nThread failure, exiting...")
		sys.exit()
	#print(status['return_code'])


def execute_command(arg):

	"""
	Takes command and arguments in a dictionary and returns the status back
	"""
	cmd = arg['cmd']
	args = shlex.split(cmd)
	#my_arg = pprint.pformat(arg)
	json_string = json.dumps(arg)
	logging.debug("Executing with the following arguments:\n"+json_string)
	
	out = ''
	err = ''
	return_code = -1
	process = None
	
	try:
		process = subprocess.Popen(
			args, 
			stdin=subprocess.PIPE,
			stdout=subprocess.PIPE,
			stderr=subprocess.PIPE,
			text=True,
			universal_newlines=True,
		)
	except FileNotFoundError as e:
		#err = "".join(process.stderr.readlines())
		err = 'Command failed, exception returned:'+format(e)+',exiting....'
		logging.critical(err)
		#sys.exit()
	except:
		logging.critical("Error, Exception not handled , exiting...")
		sys.exit(-1)
	
	if process is not None:
		return_code = process.poll()
	else:
		logging.critical('processing failed, preparing to exit...')
		sys.exit(-1)
		
	
	while True:
		#output = process.stdout.readline()
		#logging.info(output.strip())
		# Do something else, grepping et.c....
		return_code = process.poll()
		
		if return_code is not None:
			logging.info('RETURN CODE:'+ str(return_code))
			# Process has finished, read rest of the output
			out = "".join(process.stdout.readlines())
			err = "".join(process.stderr.readlines())
			logging.info("Command returned\n"+"\nout ==> "+out)
			if err != '':
				logging.info("Command returned\n"+"\nerr ==> "+err)
				print("ERROR ==> "+err)
			
			if return_code != 0:
				logging.critical('Command failed, please check messages above')
			
			break
	
	return {
		'output' : out+"\n"+err,
		'return_code' : return_code
	}

def parse_inputs():
	parser = argparse.ArgumentParser(
		description='Tool to load data from teradata to snowflake using multithreading')
	parser.add_argument('-config', action='store', type=str, required=True,
		help='snowtool Config file path')
		
	#args = parser.parse_args()
	args = vars(parser.parse_args())
	config_file = args['config']
	
	f = open(config_file)
	json_args = json.load(f)
	f.close()
	return json_args
	

if __name__ == '__main__':
	input_args = parse_inputs()
	setup_logging(input_args)
	main(input_args)
